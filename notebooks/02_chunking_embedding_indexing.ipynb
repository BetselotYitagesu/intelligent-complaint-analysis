{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4afd35",
   "metadata": {},
   "source": [
    "# Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
    "\n",
    "In this notebook, I:\n",
    "- Load the cleaned complaint dataset\n",
    "- Chunk the narratives into manageable pieces\n",
    "- Add product metadata to each chunk\n",
    "- Generate semantic embeddings\n",
    "- Store them in a FAISS vector store for efficient retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf2685e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86a287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 478834 records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_narrative</th>\n",
       "      <th>Mapped Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a xxxx xxxx card was opened under my name by a...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i made the mistake of using my wellsfargo debi...</td>\n",
       "      <td>Savings account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dear cfpb i have a secured credit card with ci...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have a citi rewards cards the credit balance...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bi am writing to dispute the following charges...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   cleaned_narrative   Mapped Product\n",
       "0  a xxxx xxxx card was opened under my name by a...      Credit card\n",
       "1  i made the mistake of using my wellsfargo debi...  Savings account\n",
       "2  dear cfpb i have a secured credit card with ci...      Credit card\n",
       "3  i have a citi rewards cards the credit balance...      Credit card\n",
       "4  bi am writing to dispute the following charges...      Credit card"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load hte filtered compliant data\n",
    "\n",
    "df = pd.read_csv(\"../data/filtered_complaints.csv\")\n",
    "print(f\"Loaded {len(df)} records\")\n",
    "df[['cleaned_narrative', 'Mapped Product']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62472b52",
   "metadata": {},
   "source": [
    "### Step 1: Text Chunking\n",
    "\n",
    "I'll use LangChain's `RecursiveCharacterTextSplitter` to break down long complaint narratives.  \n",
    "Chunking improves embedding quality and ensures we stay within token limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ea2edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks from first record: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a xxxx xxxx card was opened under my name by a fraudster i received a notice from xxxx that an account was just opened under my name i reached out to xxxx xxxx to state that this activity was unauthorized and not me xxxx xxxx confirmed this was fraudulent and immediately closed the card however they have failed to remove this from the three credit agencies and this fraud is now impacting my credit score based on a hard credit pull done by xxxx xxxx that was done by a fraudster']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Just to check example output\n",
    "example_chunks = text_splitter.split_text(df.iloc[0]['cleaned_narrative'])\n",
    "print(f\"Chunks from first record: {len(example_chunks)}\")\n",
    "example_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740de264",
   "metadata": {},
   "source": [
    "## Step 2: Add Metadata to Chunks\n",
    "\n",
    "Each chunk will carry metadata such as:\n",
    "- Product category\n",
    "- Complaint ID\n",
    "\n",
    "This will be stored along with the embedding in the vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db3d2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 478834/478834 [04:55<00:00, 1618.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total document chunks: 1378199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    chunks = text_splitter.split_text(row['cleaned_narrative'])\n",
    "    for chunk in chunks:\n",
    "        docs.append({\n",
    "            \"content\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"product\": row['Mapped Product'],\n",
    "                \"complaint_id\": row['Complaint ID']\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"Total document chunks: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f24130",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings\n",
    "\n",
    "I'll use `all-MiniLM-L6-v2` from the `sentence-transformers` library to convert text chunks into embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01956af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a840c1a1b104a7b8f80ecfccf0f00d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  41%|####      | 62.9M/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251202d1e6374656b9e293265d7972f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  41%|####      | 62.9M/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abf726d43e94f82877b94c2d5850a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  45%|####4     | 73.4M/164M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701a6916baf14b358db47135ad077df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe27f7c59973492ab35ad5f3cf6c8b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3e14082d194f29a1c187b3180d94c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9be67fb02d4994972ace1f133fc043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a805e0c5a744abb59bc089af42ce14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fd6e63273b4bcbb91794de05639217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/43069 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "texts = [doc[\"content\"] for doc in docs]\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fb1a2",
   "metadata": {},
   "source": [
    "## Step 4: Store in FAISS Vector Database\n",
    "\n",
    "I'll store the embeddings in FAISS and save the corresponding metadata for retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43125d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"../vector_store\", exist_ok=True)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"../vector_store/faiss_index.index\")\n",
    "\n",
    "# Save metadata\n",
    "with open(\"../vector_store/metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump([doc[\"metadata\"] for doc in docs], f)\n",
    "\n",
    "print(\"✅ Embeddings and metadata saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39374a0",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "\n",
    "- Loaded and processed: `filtered_complaints.csv` with 478K records\n",
    "- Generated ≈ 1378K text chunks\n",
    "- Used Sentence Transformers (`all-MiniLM-L6-v2`) to generate embeddings\n",
    "- Stored in FAISS index with associated metadata for semantic retrieval\n",
    "\n",
    "Next step: Build the RAG pipeline using these embeddings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
