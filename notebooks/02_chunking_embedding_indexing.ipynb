{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4afd35",
   "metadata": {},
   "source": [
    "## Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
    "\n",
    "In this notebook, I:\n",
    "- Load the cleaned complaint dataset\n",
    "- Chunk the narratives into manageable pieces\n",
    "- Add product metadata to each chunk\n",
    "- Generate semantic embeddings\n",
    "- Store them in a FAISS vector store for efficient retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2685e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc86a287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 478834 records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_narrative</th>\n",
       "      <th>Mapped Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a xxxx xxxx card was opened under my name by a...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i made the mistake of using my wellsfargo debi...</td>\n",
       "      <td>Savings account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dear cfpb i have a secured credit card with ci...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have a citi rewards cards the credit balance...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bi am writing to dispute the following charges...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   cleaned_narrative   Mapped Product\n",
       "0  a xxxx xxxx card was opened under my name by a...      Credit card\n",
       "1  i made the mistake of using my wellsfargo debi...  Savings account\n",
       "2  dear cfpb i have a secured credit card with ci...      Credit card\n",
       "3  i have a citi rewards cards the credit balance...      Credit card\n",
       "4  bi am writing to dispute the following charges...      Credit card"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load hte filtered compliant data\n",
    "\n",
    "df = pd.read_csv(\"../data/filtered_complaints.csv\")\n",
    "print(f\"Loaded {len(df)} records\")\n",
    "df[['cleaned_narrative', 'Mapped Product']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62472b52",
   "metadata": {},
   "source": [
    "### Step 1: Text Chunking\n",
    "\n",
    "I'll use LangChain's `RecursiveCharacterTextSplitter` to break down long complaint narratives.  \n",
    "Chunking improves embedding quality and ensures we stay within token limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ea2edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks from first record: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a xxxx xxxx card was opened under my name by a fraudster i received a notice from xxxx that an account was just opened under my name i reached out to xxxx xxxx to state that this activity was unauthorized and not me xxxx xxxx confirmed this was fraudulent and immediately closed the card however they have failed to remove this from the three credit agencies and this fraud is now impacting my credit score based on a hard credit pull done by xxxx xxxx that was done by a fraudster']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chincking the first record only!!\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Just to check example output\n",
    "example_chunks = text_splitter.split_text(df.iloc[0]['cleaned_narrative'])\n",
    "print(f\"Chunks from first record: {len(example_chunks)}\")\n",
    "example_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740de264",
   "metadata": {},
   "source": [
    "## Step 2: Add Metadata to Chunks\n",
    "\n",
    "Each chunk will carry metadata such as:\n",
    "- Product category\n",
    "- Complaint ID\n",
    "\n",
    "This will be stored along with the embedding in the vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db3d2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 478834/478834 [05:05<00:00, 1565.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total document chunks: 1378199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    chunks = text_splitter.split_text(row['cleaned_narrative'])\n",
    "    for chunk in chunks:\n",
    "        docs.append({\n",
    "            \"content\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"product\": row['Mapped Product'],\n",
    "                \"complaint_id\": row['Complaint ID']\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"Total document chunks: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f24130",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings\n",
    "\n",
    "I'll use `all-MiniLM-L6-v2` from the `sentence-transformers` library to convert text chunks into embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01956af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e3705e29a94cf180b9fbd6060c8d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (1378199, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Extract text content from docs\n",
    "texts = [doc[\"content\"] for doc in docs]\n",
    "\n",
    "# Encode with batch_size=64\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=64,              # Set batch size to 64\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True        # Ensures output is a NumPy array\n",
    ")\n",
    "\n",
    "# Show the final shape of the embeddings matrix\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fb1a2",
   "metadata": {},
   "source": [
    "## Step 4: Store in FAISS Vector Database\n",
    "\n",
    "I'll store the embeddings in FAISS and save the corresponding metadata for retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39374a0",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "\n",
    "- Loaded and processed: `filtered_complaints.csv` with 478K records\n",
    "- Generated ≈ 1378K text chunks\n",
    "- Used Sentence Transformers (`all-MiniLM-L6-v2`) to generate embeddings\n",
    "- Stored in FAISS index with associated metadata for semantic retrieval\n",
    "\n",
    "Next step: Build the RAG pipeline using these embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "752ca932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bd2e88adc34b58a4e8133fbe299623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedded 2000 chunks.\n",
      "Embedding shape: (2000, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the sentence embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Limit to first 2000 chunks only (adjustable)\n",
    "max_chunks = 2000\n",
    "limited_docs = docs[:max_chunks]\n",
    "\n",
    "# Extract text content from the limited docs\n",
    "texts = [doc[\"content\"] for doc in limited_docs]\n",
    "\n",
    "# Generate embeddings with a progress bar\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True  # Ensures output is a NumPy array\n",
    ")\n",
    "\n",
    "# Display the resulting shape\n",
    "print(f\"✅ Embedded {len(texts)} chunks.\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa4b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embeddings_2000.npy\", embeddings)\n",
    "\n",
    "import pickle\n",
    "with open(\"docs_2000.pkl\", \"wb\") as f:\n",
    "    pickle.dump(limited_docs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c5e810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built with 2000 vectors.\n",
      "FAISS index and metadata saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Dimension of embeddings\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Initialize a FAISS index (L2 distance)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# Save the FAISS index to disk\n",
    "faiss.write_index(index, \"../vector_store/index.faiss\")\n",
    "\n",
    "# Save the corresponding metadata (docs) as a pickle file\n",
    "with open(\"../vector_store/index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(limited_docs, f)\n",
    "\n",
    "print(\"FAISS index and metadata saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24de00b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index with 2000 vectors.\n",
      "Loaded 2000 documents metadata.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Load the index\n",
    "index = faiss.read_index(\"../vector_store/index.faiss\")\n",
    "\n",
    "# Load metadata\n",
    "with open(\"../vector_store/index.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded FAISS index with {index.ntotal} vectors.\")\n",
    "print(f\"Loaded {len(docs)} documents metadata.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7a78ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: the xxxx of xxxx this is an egregious act that not only blindsided me but manipulates my account to make it appear that im close to reaching the limit when in fact i was no where near the limit this c...\n",
      "Result 2: i have a fidelity rewards visa signature credit card with fidelity investments ending in xxxx when i was first approved for the card i was given a credit limit of xxxx since then i have been responsib...\n",
      "Result 3: for the card ending in xxxx this is a great inconvenience and turnoff to me the account holder and customer if fidelity wants to maintain a relationship with me or other consumers for business in the ...\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def search(query, k=5):\n",
    "    # Embed query\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve corresponding docs\n",
    "    results = [docs[idx] for idx in indices[0] if idx != -1]\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "results = search(\"What is my credit card limit?\", k=3)\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"Result {i+1}: {res['content'][:200]}...\")  # print first 200 chars\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
